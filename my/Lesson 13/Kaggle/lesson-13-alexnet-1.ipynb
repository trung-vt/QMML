{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7692237,"sourceType":"datasetVersion","datasetId":4489278}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"%env PYTORCH_ENABLE_MPS_FALLBACK=1","metadata":{"ExecuteTime":{"end_time":"2024-02-19T17:02:32.177401Z","start_time":"2024-02-19T17:02:32.175399Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-24T14:27:48.470560Z","iopub.execute_input":"2024-02-24T14:27:48.470802Z","iopub.status.idle":"2024-02-24T14:27:48.482811Z","shell.execute_reply.started":"2024-02-24T14:27:48.470779Z","shell.execute_reply":"2024-02-24T14:27:48.481853Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: PYTORCH_ENABLE_MPS_FALLBACK=1\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar -xf /kaggle/input/imagenette2-160/imagenette2-160.tgz","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:32:11.134583Z","iopub.execute_input":"2024-02-24T14:32:11.134959Z","iopub.status.idle":"2024-02-24T14:32:14.187848Z","shell.execute_reply.started":"2024-02-24T14:32:11.134928Z","shell.execute_reply":"2024-02-24T14:32:14.186316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:32:24.646292Z","iopub.execute_input":"2024-02-24T14:32:24.647256Z","iopub.status.idle":"2024-02-24T14:32:25.657893Z","shell.execute_reply.started":"2024-02-24T14:32:24.647218Z","shell.execute_reply":"2024-02-24T14:32:25.656725Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Set the environment variable to enable CPU fallback for MPS\nos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'","metadata":{"ExecuteTime":{"end_time":"2024-02-19T17:02:33.064349Z","start_time":"2024-02-19T17:02:33.060452Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-24T14:27:48.484471Z","iopub.execute_input":"2024-02-24T14:27:48.485026Z","iopub.status.idle":"2024-02-24T14:27:48.493594Z","shell.execute_reply.started":"2024-02-24T14:27:48.484993Z","shell.execute_reply":"2024-02-24T14:27:48.492707Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# AlexNet - The paper which started the deep learning revolution!!!\n\nAlexNet is a seminal paper in the field of deep learning, it was one of the first deep neural networks trained in the wild on a real\nproblem. ImageNet is the dataset this model trained on and the dataset is a classification task, with 1.2 million images and 1000 classes.\n\nWhen AlexNet was released it changed the computer vision game, shifting focus from handcrafted specific methods to more general CV \nmethods, i.e. neural networks. It was 10.3% ahead in top-1 against the next best competitor (top-1 means of the classes how many did it\nget correct).\n\nIn this session I will provide you with an AlexNet cookbook to help you implement the model.\n\nLink to the paper: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf","metadata":{}},{"cell_type":"markdown","source":"## Boiler plate code blocks","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\n\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision.transforms as transforms\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"ExecuteTime":{"end_time":"2024-02-19T17:02:36.851310Z","start_time":"2024-02-19T17:02:35.382885Z"},"execution":{"iopub.status.busy":"2024-02-24T14:27:48.495311Z","iopub.execute_input":"2024-02-24T14:27:48.496048Z","iopub.status.idle":"2024-02-24T14:27:54.910408Z","shell.execute_reply.started":"2024-02-24T14:27:48.496015Z","shell.execute_reply":"2024-02-24T14:27:54.909417Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if torch.backends.mps.is_available():\n\tdevice = torch.device(\"mps\")\n\tx = torch.ones(1, device=device)\n\tprint(x)\n\nelif torch.backends.cuda.is_built():\n\tdevice = torch.device(\"cuda\")\n\tx = torch.ones(1, device=device)\n\tprint (x)\nelse:\n\tdevice = None\n\tprint (\"MPS device not found.\")","metadata":{"ExecuteTime":{"end_time":"2024-02-19T17:02:37.012902Z","start_time":"2024-02-19T17:02:36.870112Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-24T14:27:54.911621Z","iopub.execute_input":"2024-02-24T14:27:54.912007Z","iopub.status.idle":"2024-02-24T14:27:55.367395Z","shell.execute_reply.started":"2024-02-24T14:27:54.911982Z","shell.execute_reply":"2024-02-24T14:27:55.366186Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tensor([1.], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We will be using the ImageNette dataset (a subset of ImageNet as ImageNet is around 125gb so it's not feasible to run without a GPU and a lot of time)\n\nTo download ImageNette run these following commands:\n\nmkdir Data && cd data \nwget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\ntar -xvf imagenette2-160.tgz \n","metadata":{}},{"cell_type":"markdown","source":"# The AlexNet CookBook\n<h2><b> For today's session I will provide you with a list of ingredients, which will be all the layers you need </b></h2>\n\nBeware some things are missing from this cookbook and you will need to utilise the paper to figure out the exact details.\n\nAlexNet Implementation Ingredients list:\n\n** n represents the number of total layers, so if n = 3 then there will be 3 of those layers **\n\n- Convolutional layers:\n\t- conv1 - 11x11 @ 96 \n\t- conv2 - 5x5 @ 256\n\t- conv3 - 3x3 @ 384\n\t- conv4 - 3x3 @ 384\n\t- conv5 - 3x3 @ 256\n\n- Fully connected layers:\n\t- fc1 - in_features = ?, out_features = 4096\n\t- fc2 - in_features = 4096, out_features = 4096\n\t- fc3 - in_features = 4096, out_features = 1000\n\n- Non-Linear activation layers:\n\t- relu_n - ReLU layers follow every layer\n\n- Norm layers: \n\t- LRN_n - Follows specific layers, check the paper for more details or ask me :) - size=?, alpha=?, beta=?, k=?\n\n- maxpool layers:\n\t- mp_n - Once again follows specific layers, check the paper for more details or ask me :)\n\nAnd that is all you need to construct AlexNet!!!","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\tdef __init__(self):\n\t\tsuper().__init__()\n\n\t\tself.conv1 = nn.Conv2d(\n\t\t\tkernel_size=(11, 11),\n\t\t\tin_channels=3,\n\t\t\tout_channels=96,\n\t\t\tstride=4,\n\t\t\t# padding=5,\n\t\t)\n\t\tself.relu1 = nn.ReLU()\n\t\tself.LRN1 = nn.LocalResponseNorm(\n\t\t\tsize=5, alpha=10**(-4), beta=0.75, k=2\n\t\t)\n\t\t# output 1\n\n\t\tself.maxpool1 = nn.MaxPool2d(\n\t\t\tkernel_size=(3, 3),\n\t\t\tstride=(2, 2),\n\t\t)\n\t\tself.conv2 = nn.Conv2d(\n\t\t\tkernel_size=(5, 5),\n\t\t\tin_channels=96,\n\t\t\tout_channels=256,\n\t\t\tpadding=2,\n\t\t)\n\t\tself.relu2 = nn.ReLU()\n\t\tself.LRN2 = nn.LocalResponseNorm(\n\t\t\tsize=5, alpha=10**(-4), beta=0.75, k=2\n\t\t)\n\t\t# output 2\n\n\t\tself.maxpool2 = nn.MaxPool2d(\n\t\t\tkernel_size=(3, 3),\n\t\t\tstride=(2, 2),\n\t\t\t# stride=2,\n\t\t)\n\t\tself.conv3 = nn.Conv2d(\n\t\t\tkernel_size=(3, 3),\n\t\t\tin_channels=256,\n\t\t\tout_channels=384,\n\t\t\tpadding=1,\n\t\t)\n\t\tself.relu3 = nn.ReLU()\n\t\t# output 3\n\n\t\tself.conv4 = nn.Conv2d(\n\t\t\tkernel_size=(3, 3),\n\t\t\tin_channels=384,\n\t\t\tout_channels=384,\n\t\t\tpadding=1\n\t\t)\n\t\tself.relu4 = nn.ReLU()\n\t\t# output 4\n\n\t\tself.conv5 = nn.Conv2d(\n\t\t\tkernel_size=(3, 3),\n\t\t\tin_channels=384,\n\t\t\tout_channels=256,\n\t\t\tpadding=1\n\t\t)\n\t\tself.relu5 = nn.ReLU()\n\t\tself.maxpool3 = nn.MaxPool2d(\n\t\t\tkernel_size=(3, 3),\n\t\t\tstride=(2, 2),\n\t\t\t# stride=2\n\t\t)\n\t\t# output 5\n\n\t\tself.fc1 = nn.Linear(9216, 4096)\n\t\tself.relu6 = nn.ReLU()\n\t\tself.dropout1 = nn.Dropout(p=0.5)\n\n\t\tself.fc2 = nn.Linear(4096, 4096)\n\t\tself.relu7 = nn.ReLU()\n\t\tself.dropout2 = nn.Dropout(p=0.5)\n\n\t\tself.fc3 = nn.Linear(4096, 1000)\n# \t\tself.fc3 = nn.Linear(4096, 10)  # If using CIFAR10\n\t\t# self.relu8 = nn.ReLU() is this supposed to be here?\n\t\tself.softmax = nn.Softmax()\n\n\tdef forward(self, x):\n\t\t# This block represents first convolution\n\t\tx = self.conv1(x)\n# \t\tprint(f'Shape after conv1: {x.shape}')\n\t\tx = self.relu1(x)\n# \t\tprint(f'Shape after ReLU1: {x.shape}')\n\t\tx = self.LRN1(x)\n# \t\tprint(f'Shape after LRN1: {x.shape}')\n\t\tx = self.maxpool1(x)\n# \t\tprint(f'Shape after MP1: {x.shape}\\n')\n\n\t\t# Second convolution\n\t\tx = self.conv2(x)\n# \t\tprint(f'Shape after conv2: {x.shape}')\n\t\tx = self.relu2(x)\n# \t\tprint(f'Shape after ReLU2: {x.shape}')\n\t\tx = self.LRN2(x)\n# \t\tprint(f'Shape after LRN2: {x.shape}')\n\t\tx = self.maxpool2(x)\n# \t\tprint(f'Shape after MP2: {x.shape}\\n')\n\n\t\t# Third convolution\n\t\tx = self.conv3(x)\n# \t\tprint(f'Shape after conv3: {x.shape}')\n\t\tx = self.relu3(x)\n# \t\tprint(f'Shape after ReLU3: {x.shape}\\n')\n\n\t\t# Fourth convolution\n\t\tx = self.conv4(x)\n# \t\tprint(f'Shape after conv4: {x.shape}')\n\t\tx = self.relu4(x)\n# \t\tprint(f'Shape after relu4: {x.shape}\\n')\n\n\t\t# Fifth convolution\n\t\tx = self.conv5(x)\n# \t\tprint(f'Shape after conv5: {x.shape}')\n\t\tx = self.relu5(x)\n# \t\tprint(f'Shape after relu5: {x.shape}')\n\t\tx = self.maxpool3(x)\n# \t\tprint(f'Shape after MP3: {x.shape}\\n')\n\n\t\t# Before passing to the fully connected layer we must flatten the tensor as nn.Linear expects a matrix (2d input)\n\t\tx = torch.flatten(x, start_dim=1)\n# \t\tprint(x.shape)\n\n\t\t# First fully connected layer\n\t\tx = self.fc1(x)\n# \t\tprint(f'Shape after fc1: {x.shape}')\n\t\tx = self.relu6(x)\n# \t\tprint(f'Shape after relu6: {x.shape}')\n\t\tx = self.dropout1(x)\n# \t\tprint(f'Shape after dropout1: {x.shape}\\n')\n\n\t\t# Second fully connected layer\n\t\tx = self.fc2(x)\n# \t\tprint(f'Shape after fc2: {x.shape}')\n\t\tx = self.relu7(x)\n# \t\tprint(f'Shape after relu7: {x.shape}')\n\t\tx = self.dropout2(x)\n# \t\tprint(f'Shape after dropout2: {x.shape}\\n')\n\n\t\t# Final FC layer\n\t\tx = self.fc3(x)\n# \t\tprint(f'Shape after fc3: {x.shape}')\n\t\t# x = self.relu8(x)\n\t\t# print(f'Shape after relu8: {x.shape}')\n# \t\tx = self.softmax(x)\n# \t\tprint(f'Shape after softmax: {x.shape}\\n')\n\n\t\treturn x\n\nmodel = AlexNet()\n\nif device is not None:\n\tmodel.to(device)","metadata":{"ExecuteTime":{"end_time":"2024-02-19T17:02:40.026425Z","start_time":"2024-02-19T17:02:39.527850Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-24T14:48:06.629600Z","iopub.execute_input":"2024-02-24T14:48:06.630048Z","iopub.status.idle":"2024-02-24T14:48:07.545198Z","shell.execute_reply.started":"2024-02-24T14:48:06.630003Z","shell.execute_reply":"2024-02-24T14:48:07.544365Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Define transformations\ntransform = transforms.Compose(\n    [transforms.Resize((227, 227)),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nTRAIN_DATA_DIR = 'imagenette2-160/train'\n# TRAIN_DATA_DIR = './Data/imagenette2-160/train'\nTEST_DATA_DIR = 'imagenette2-160/val'\n# TEST_DATA_DIR = './Data/imagenette2-160/val'\n\n# Load ImageNette dataset\ntrainset = torchvision.datasets.ImageFolder(\n        TRAIN_DATA_DIR, transform=transform\n    )\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n\n\ntestset= torchvision.datasets.ImageFolder(\n        TEST_DATA_DIR, transform=transform\n    )\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)","metadata":{"ExecuteTime":{"end_time":"2024-02-19T17:02:40.521083Z","start_time":"2024-02-19T17:02:40.442557Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-24T14:33:59.201082Z","iopub.execute_input":"2024-02-24T14:33:59.201948Z","iopub.status.idle":"2024-02-24T14:33:59.292704Z","shell.execute_reply.started":"2024-02-24T14:33:59.201919Z","shell.execute_reply":"2024-02-24T14:33:59.291841Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005, weight_decay=0.0005, momentum=0.9)\n\n# Define learning rate scheduler\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n\nimport time\n\nfor epoch in range(35):\n    print(f'Epoch [{epoch + 1}/{35}] starts...')\n    start_time = time.time()\n\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    train_loss = running_loss / len(train_loader)\n    print(f'Epoch [{epoch + 1}] training loss: {train_loss:.3f}')\n\n    # Validation phase\n    model.eval()  # Set model to evaluation mode\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for data in test_loader:  # Assuming test_loader is used as a validation loader\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            val_running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    val_loss = val_running_loss / len(test_loader)\n    val_accuracy = 100 * val_correct / val_total\n    print(f'Epoch [{epoch + 1}] validation loss: {val_loss:.3f}, accuracy: {val_accuracy:.2f}%')\n\n    # Update the LR scheduler with validation loss\n    scheduler.step(val_loss)\n#     print(f'LR: {scheduler.get_last_lr()}')\n\n    end_time = time.time()\n    print(f'Epoch [{epoch + 1}/{35}] ends. Time taken: {end_time - start_time:.2f} seconds\\n')","metadata":{"ExecuteTime":{"end_time":"2024-02-19T17:07:37.883266Z","start_time":"2024-02-19T17:02:41.821275Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-24T14:37:24.378755Z","iopub.execute_input":"2024-02-24T14:37:24.379131Z","iopub.status.idle":"2024-02-24T14:48:06.626879Z","shell.execute_reply.started":"2024-02-24T14:37:24.379100Z","shell.execute_reply":"2024-02-24T14:48:06.625650Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch [1/35] starts...\nEpoch [1] training loss: 2.303\nEpoch [1] validation loss: 2.303, accuracy: 9.10%\nEpoch [1/35] ends. Time taken: 18.65 seconds\n\nEpoch [2/35] starts...\nEpoch [2] training loss: 2.302\nEpoch [2] validation loss: 2.303, accuracy: 9.10%\nEpoch [2/35] ends. Time taken: 18.24 seconds\n\nEpoch [3/35] starts...\nEpoch [3] training loss: 2.301\nEpoch [3] validation loss: 2.301, accuracy: 9.10%\nEpoch [3/35] ends. Time taken: 18.41 seconds\n\nEpoch [4/35] starts...\nEpoch [4] training loss: 2.293\nEpoch [4] validation loss: 2.280, accuracy: 11.69%\nEpoch [4/35] ends. Time taken: 18.52 seconds\n\nEpoch [5/35] starts...\nEpoch [5] training loss: 2.209\nEpoch [5] validation loss: 2.108, accuracy: 22.88%\nEpoch [5/35] ends. Time taken: 18.25 seconds\n\nEpoch [6/35] starts...\nEpoch [6] training loss: 2.008\nEpoch [6] validation loss: 1.957, accuracy: 31.49%\nEpoch [6/35] ends. Time taken: 18.41 seconds\n\nEpoch [7/35] starts...\nEpoch [7] training loss: 1.928\nEpoch [7] validation loss: 1.892, accuracy: 33.27%\nEpoch [7/35] ends. Time taken: 18.22 seconds\n\nEpoch [8/35] starts...\nEpoch [8] training loss: 1.822\nEpoch [8] validation loss: 1.812, accuracy: 34.60%\nEpoch [8/35] ends. Time taken: 18.55 seconds\n\nEpoch [9/35] starts...\nEpoch [9] training loss: 1.722\nEpoch [9] validation loss: 1.748, accuracy: 40.61%\nEpoch [9/35] ends. Time taken: 18.31 seconds\n\nEpoch [10/35] starts...\nEpoch [10] training loss: 1.588\nEpoch [10] validation loss: 1.546, accuracy: 46.88%\nEpoch [10/35] ends. Time taken: 18.27 seconds\n\nEpoch [11/35] starts...\nEpoch [11] training loss: 1.505\nEpoch [11] validation loss: 1.475, accuracy: 50.57%\nEpoch [11/35] ends. Time taken: 18.54 seconds\n\nEpoch [12/35] starts...\nEpoch [12] training loss: 1.388\nEpoch [12] validation loss: 1.401, accuracy: 52.41%\nEpoch [12/35] ends. Time taken: 18.04 seconds\n\nEpoch [13/35] starts...\nEpoch [13] training loss: 1.297\nEpoch [13] validation loss: 1.280, accuracy: 57.02%\nEpoch [13/35] ends. Time taken: 18.32 seconds\n\nEpoch [14/35] starts...\nEpoch [14] training loss: 1.206\nEpoch [14] validation loss: 1.193, accuracy: 60.05%\nEpoch [14/35] ends. Time taken: 18.17 seconds\n\nEpoch [15/35] starts...\nEpoch [15] training loss: 1.137\nEpoch [15] validation loss: 1.168, accuracy: 61.66%\nEpoch [15/35] ends. Time taken: 18.38 seconds\n\nEpoch [16/35] starts...\nEpoch [16] training loss: 1.073\nEpoch [16] validation loss: 1.066, accuracy: 64.74%\nEpoch [16/35] ends. Time taken: 18.04 seconds\n\nEpoch [17/35] starts...\nEpoch [17] training loss: 0.990\nEpoch [17] validation loss: 1.135, accuracy: 63.75%\nEpoch [17/35] ends. Time taken: 18.40 seconds\n\nEpoch [18/35] starts...\nEpoch [18] training loss: 0.923\nEpoch [18] validation loss: 1.056, accuracy: 64.82%\nEpoch [18/35] ends. Time taken: 18.48 seconds\n\nEpoch [19/35] starts...\nEpoch [19] training loss: 0.851\nEpoch [19] validation loss: 0.997, accuracy: 68.03%\nEpoch [19/35] ends. Time taken: 18.10 seconds\n\nEpoch [20/35] starts...\nEpoch [20] training loss: 0.806\nEpoch [20] validation loss: 0.937, accuracy: 70.42%\nEpoch [20/35] ends. Time taken: 18.47 seconds\n\nEpoch [21/35] starts...\nEpoch [21] training loss: 0.731\nEpoch [21] validation loss: 0.990, accuracy: 70.01%\nEpoch [21/35] ends. Time taken: 18.10 seconds\n\nEpoch [22/35] starts...\nEpoch [22] training loss: 0.693\nEpoch [22] validation loss: 0.901, accuracy: 72.03%\nEpoch [22/35] ends. Time taken: 18.32 seconds\n\nEpoch [23/35] starts...\nEpoch [23] training loss: 0.630\nEpoch [23] validation loss: 0.953, accuracy: 70.04%\nEpoch [23/35] ends. Time taken: 18.11 seconds\n\nEpoch [24/35] starts...\nEpoch [24] training loss: 0.582\nEpoch [24] validation loss: 0.928, accuracy: 72.08%\nEpoch [24/35] ends. Time taken: 18.41 seconds\n\nEpoch [25/35] starts...\nEpoch [25] training loss: 0.535\nEpoch [25] validation loss: 0.938, accuracy: 71.92%\nEpoch [25/35] ends. Time taken: 18.52 seconds\n\nEpoch [26/35] starts...\nEpoch [26] training loss: 0.501\nEpoch [26] validation loss: 0.910, accuracy: 72.28%\nEpoch [26/35] ends. Time taken: 18.22 seconds\n\nEpoch [27/35] starts...\nEpoch [27] training loss: 0.314\nEpoch [27] validation loss: 0.904, accuracy: 74.80%\nEpoch [27/35] ends. Time taken: 18.36 seconds\n\nEpoch [28/35] starts...\nEpoch [28] training loss: 0.268\nEpoch [28] validation loss: 0.912, accuracy: 74.90%\nEpoch [28/35] ends. Time taken: 18.23 seconds\n\nEpoch [29/35] starts...\nEpoch [29] training loss: 0.239\nEpoch [29] validation loss: 0.951, accuracy: 75.03%\nEpoch [29/35] ends. Time taken: 18.25 seconds\n\nEpoch [30/35] starts...\nEpoch [30] training loss: 0.227\nEpoch [30] validation loss: 0.963, accuracy: 75.11%\nEpoch [30/35] ends. Time taken: 18.08 seconds\n\nEpoch [31/35] starts...\nEpoch [31] training loss: 0.202\nEpoch [31] validation loss: 0.961, accuracy: 75.26%\nEpoch [31/35] ends. Time taken: 18.37 seconds\n\nEpoch [32/35] starts...\nEpoch [32] training loss: 0.199\nEpoch [32] validation loss: 0.966, accuracy: 75.31%\nEpoch [32/35] ends. Time taken: 18.66 seconds\n\nEpoch [33/35] starts...\nEpoch [33] training loss: 0.197\nEpoch [33] validation loss: 0.969, accuracy: 75.44%\nEpoch [33/35] ends. Time taken: 18.80 seconds\n\nEpoch [34/35] starts...\nEpoch [34] training loss: 0.199\nEpoch [34] validation loss: 0.969, accuracy: 75.44%\nEpoch [34/35] ends. Time taken: 18.76 seconds\n\nEpoch [35/35] starts...\nEpoch [35] training loss: 0.192\nEpoch [35] validation loss: 0.970, accuracy: 75.34%\nEpoch [35/35] ends. Time taken: 18.27 seconds\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example code using PyTorch and torchvision\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport cv2\n\ndef test_predict():\n    # # Load pre-trained ResNet model\n    # resnet_model = models.resnet50(pretrained=True)\n    # resnet_model.eval()\n\n#     # Preprocess input image\n#     transform = transforms.Compose([\n#         transforms.Resize((224, 224)),\n#         transforms.ToTensor(),\n#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n#     ])\n\n#     # image_path = 'path_to_dog_image.jpg'\n#     image_path = 'imagenette2-160/val/n01440764/ILSVRC2012_val_00009111.JPEG'\n#     image = Image.open(image_path)\n#     input_tensor = transform(image)\n#     input_batch = input_tensor.unsqueeze(0)\n\n#     # Make prediction\n#     with torch.no_grad():\n#     #     output = resnet_model(input_batch)\n#         output = model(input_batch)\n\n\n#     # Get predicted class\n#     _, predicted_class = torch.max(output, 1)\n#     print(f\"Predicted class index: {predicted_class.item()}\")\n\n    model.eval()  # Set model to evaluation mode\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for data in test_loader:  # Assuming test_loader is used as a validation loader\n            inputs, labels = data\n            \n            print(type(data))\n            print(type(data[0]))\n            \n            tensor = data[0]\n            tensor  = tensor.cpu().numpy() # make sure tensor is on cpu\n            cv2.imwrite(\"image.png\", tensor)\n            \n            break\n            \n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            val_running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n#     val_loss = val_running_loss / len(test_loader)\n#     val_accuracy = 100 * val_correct / val_total\n#     print(f'Validation loss: {val_loss:.3f}, accuracy: {val_accuracy:.2f}%')\n\ntest_predict()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-24T15:07:57.930175Z","iopub.execute_input":"2024-02-24T15:07:57.930570Z","iopub.status.idle":"2024-02-24T15:07:58.626589Z","shell.execute_reply.started":"2024-02-24T15:07:57.930533Z","shell.execute_reply":"2024-02-24T15:07:58.625213Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"<class 'list'>\n<class 'torch.Tensor'>\n","output_type":"stream"},{"name":"stderr","text":"libpng warning: Invalid image width in IHDR\nlibpng warning: Image width exceeds user limit in IHDR\nlibpng warning: Invalid image height in IHDR\nlibpng warning: Image height exceeds user limit in IHDR\nlibpng error: Invalid IHDR data\n","output_type":"stream"}]},{"cell_type":"code","source":"help(cv2.imwrite)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:06:49.511912Z","iopub.execute_input":"2024-02-24T15:06:49.512306Z","iopub.status.idle":"2024-02-24T15:06:49.518597Z","shell.execute_reply.started":"2024-02-24T15:06:49.512273Z","shell.execute_reply":"2024-02-24T15:06:49.517654Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Help on built-in function imwrite:\n\nimwrite(...)\n    imwrite(filename, img[, params]) -> retval\n    .   @brief Saves an image to a specified file.\n    .   \n    .   The function imwrite saves the image to the specified file. The image format is chosen based on the\n    .   filename extension (see cv::imread for the list of extensions). In general, only 8-bit unsigned (CV_8U)\n    .   single-channel or 3-channel (with 'BGR' channel order) images\n    .   can be saved using this function, with these exceptions:\n    .   \n    .   - With OpenEXR encoder, only 32-bit float (CV_32F) images can be saved.\n    .     - 8-bit unsigned (CV_8U) images are not supported.\n    .   - With Radiance HDR encoder, non 64-bit float (CV_64F) images can be saved.\n    .     - All images will be converted to 32-bit float (CV_32F).\n    .   - With JPEG 2000 encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n    .   - With PAM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n    .   - With PNG encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n    .     - PNG images with an alpha channel can be saved using this function. To do this, create\n    .       8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels\n    .       should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).\n    .   - With PGM/PPM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n    .   - With TIFF encoder, 8-bit unsigned (CV_8U), 16-bit unsigned (CV_16U),\n    .                        32-bit float (CV_32F) and 64-bit float (CV_64F) images can be saved.\n    .     - Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).\n    .     - 32-bit float 3-channel (CV_32FC3) TIFF images will be saved\n    .       using the LogLuv high dynamic range encoding (4 bytes per pixel)\n    .   \n    .   If the image format is not supported, the image will be converted to 8-bit unsigned (CV_8U) and saved that way.\n    .   \n    .   If the format, depth or channel order is different, use\n    .   Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O\n    .   functions to save the image to XML or YAML format.\n    .   \n    .   The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file.\n    .   It also demonstrates how to save multiple images in a TIFF file:\n    .   @include snippets/imgcodecs_imwrite.cpp\n    .   @param filename Name of the file.\n    .   @param img (Mat or vector of Mat) Image or Images to be saved.\n    .   @param params Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see cv::ImwriteFlags\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:07:19.659436Z","iopub.execute_input":"2024-02-24T15:07:19.659821Z","iopub.status.idle":"2024-02-24T15:07:20.632719Z","shell.execute_reply.started":"2024-02-24T15:07:19.659787Z","shell.execute_reply":"2024-02-24T15:07:20.631563Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"imagenette2-160\n","output_type":"stream"}]}]}