{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f21b6fce87bf60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:32.177401Z",
     "start_time": "2024-02-19T17:02:32.175399Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76c2e0723644fa95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:33.064349Z",
     "start_time": "2024-02-19T17:02:33.060452Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Set the environment variable to enable CPU fallback for MPS\n",
    "# os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9933e11ee9687c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AlexNet - The paper which started the deep learning revolution!!!\n",
    "\n",
    "AlexNet is a seminal paper in the field of deep learning, it was one of the first deep neural networks trained in the wild on a real\n",
    "problem. ImageNet is the dataset this model trained on and the dataset is a classification task, with 1.2 million images and 1000 classes.\n",
    "\n",
    "When AlexNet was released it changed the computer vision game, shifting focus from handcrafted specific methods to more general CV \n",
    "methods, i.e. neural networks. It was 10.3% ahead in top-1 against the next best competitor (top-1 means of the classes how many did it\n",
    "get correct).\n",
    "\n",
    "In this session I will provide you with an AlexNet cookbook to help you implement the model.\n",
    "\n",
    "Link to the paper: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c3983a2eab1ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Boiler plate code blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:36.851310Z",
     "start_time": "2024-02-19T17:02:35.382885Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6830a59f993f70d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:37.012902Z",
     "start_time": "2024-02-19T17:02:36.870112Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "\tdevice = torch.device(\"mps\")\n",
    "\tx = torch.ones(1, device=device)\n",
    "\tprint(x)\n",
    "\n",
    "elif torch.backends.cuda.is_built():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\tx = torch.ones(1, device=device)\n",
    "\tprint (x)\n",
    "else:\n",
    "\tdevice = None\n",
    "\tprint (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa2e296d1467a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will be using the ImageNette dataset (a subset of ImageNet as ImageNet is around 125gb so it's not feasible to run without a GPU and a lot of time)\n",
    "\n",
    "To download ImageNette run these following commands:\n",
    "\n",
    "mkdir Data && cd data \n",
    "wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
    "tar -xvf imagenette2-160.tgz \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2add9cd31bd56cb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The AlexNet CookBook\n",
    "<h2><b> For today's session I will provide you with a list of ingredients, which will be all the layers you need </b></h2>\n",
    "\n",
    "Beware some things are missing from this cookbook and you will need to utilise the paper to figure out the exact details.\n",
    "\n",
    "AlexNet Implementation Ingredients list:\n",
    "\n",
    "** n represents the number of total layers, so if n = 3 then there will be 3 of those layers **\n",
    "\n",
    "- Convolutional layers:\n",
    "\t- conv1 - 11x11 @ 96 \n",
    "\t- conv2 - 5x5 @ 256\n",
    "\t- conv3 - 3x3 @ 384\n",
    "\t- conv4 - 3x3 @ 384\n",
    "\t- conv5 - 3x3 @ 256\n",
    "\n",
    "- Fully connected layers:\n",
    "\t- fc1 - in_features = ?, out_features = 4096\n",
    "\t- fc2 - in_features = 4096, out_features = 4096\n",
    "\t- fc3 - in_features = 4096, out_features = 1000\n",
    "\n",
    "- Non-Linear activation layers:\n",
    "\t- relu_n - ReLU layers follow every layer\n",
    "\n",
    "- Norm layers: \n",
    "\t- LRN_n - Follows specific layers, check the paper for more details or ask me :) - size=?, alpha=?, beta=?, k=?\n",
    "\n",
    "- maxpool layers:\n",
    "\t- mp_n - Once again follows specific layers, check the paper for more details or ask me :)\n",
    "\n",
    "And that is all you need to construct AlexNet!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d524cba87ac5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:40.026425Z",
     "start_time": "2024-02-19T17:02:39.527850Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(11, 11),\n",
    "\t\t\tin_channels=3,\n",
    "\t\t\tout_channels=96,\n",
    "\t\t\tstride=4,i\n",
    "\t\t\t# padding=5,\n",
    "\t\t)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\tself.LRN1 = nn.LocalResponseNorm(\n",
    "\t\t\tsize=5, alpha=10**(-4), beta=0.75, k=2\n",
    "\t\t)\n",
    "\t\t# output 1\n",
    "\n",
    "\t\tself.maxpool1 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=(2, 2),\n",
    "\t\t)\n",
    "\t\tself.conv2 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(5, 5),\n",
    "\t\t\tin_channels=96,\n",
    "\t\t\tout_channels=256,\n",
    "\t\t\tpadding=2,\n",
    "\t\t)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\t\tself.LRN2 = nn.LocalResponseNorm(\n",
    "\t\t\tsize=5, alpha=10**(-4), beta=0.75, k=2\n",
    "\t\t)\n",
    "\t\t# output 2\n",
    "\n",
    "\t\tself.maxpool2 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=(2, 2),\n",
    "\t\t\t# stride=2,\n",
    "\t\t)\n",
    "\t\tself.conv3 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=256,\n",
    "\t\t\tout_channels=384,\n",
    "\t\t\tpadding=1,\n",
    "\t\t)\n",
    "\t\tself.relu3 = nn.ReLU()\n",
    "\t\t# output 3\n",
    "\n",
    "\t\tself.conv4 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=384,\n",
    "\t\t\tout_channels=384,\n",
    "\t\t\tpadding=1\n",
    "\t\t)\n",
    "\t\tself.relu4 = nn.ReLU()\n",
    "\t\t# output 4\n",
    "\n",
    "\t\tself.conv5 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=384,\n",
    "\t\t\tout_channels=256,\n",
    "\t\t\tpadding=1\n",
    "\t\t)\n",
    "\t\tself.relu5 = nn.ReLU()\n",
    "\t\tself.maxpool3 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=(2, 2),\n",
    "\t\t\t# stride=2\n",
    "\t\t)\n",
    "\t\t# output 5\n",
    "\n",
    "\t\tself.fc1 = nn.Linear(9216, 4096)\n",
    "\t\tself.relu6 = nn.ReLU()\n",
    "\t\tself.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "\t\tself.fc2 = nn.Linear(4096, 4096)\n",
    "\t\tself.relu7 = nn.ReLU()\n",
    "\t\tself.dropout2 = nn.Dropout(p=0.5)\n",
    "\n",
    "\t\tself.fc3 = nn.Linear(4096, 1000)\n",
    "\t\t# self.fc3 = nn.Linear(4096, 10)  # If using CIFAR10\n",
    "\t\t# self.relu8 = nn.ReLU() is this supposed to be here?\n",
    "\t\tself.softmax = nn.Softmax()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# This block represents first convolution\n",
    "\t\tx = self.conv1(x)\n",
    "\t\t# print(f'Shape after conv1: {x.shape}')\n",
    "\t\tx = self.relu1(x)\n",
    "\t\t# print(f'Shape after ReLU1: {x.shape}')\n",
    "\t\tx = self.LRN1(x)\n",
    "\t\t# print(f'Shape after LRN1: {x.shape}')\n",
    "\t\tx = self.maxpool1(x)\n",
    "\t\t# print(f'Shape after MP1: {x.shape}\\n')\n",
    "\n",
    "\t\t# Second convolution\n",
    "\t\tx = self.conv2(x)\n",
    "\t\t# print(f'Shape after conv2: {x.shape}')\n",
    "\t\tx = self.relu2(x)\n",
    "\t\t# print(f'Shape after ReLU2: {x.shape}')\n",
    "\t\tx = self.LRN2(x)\n",
    "\t\t# print(f'Shape after LRN2: {x.shape}')\n",
    "\t\tx = self.maxpool2(x)\n",
    "\t\t# print(f'Shape after MP2: {x.shape}\\n')\n",
    "\n",
    "\t\t# Third convolution\n",
    "\t\tx = self.conv3(x)\n",
    "\t\t# print(f'Shape after conv3: {x.shape}')\n",
    "\t\tx = self.relu3(x)\n",
    "\t\t# print(f'Shape after ReLU3: {x.shape}\\n')\n",
    "\n",
    "\t\t# Fourth convolution\n",
    "\t\tx = self.conv4(x)\n",
    "\t\t# print(f'Shape after conv4: {x.shape}')\n",
    "\t\tx = self.relu4(x)\n",
    "\t\t# print(f'Shape after relu4: {x.shape}\\n')\n",
    "\n",
    "\t\t# Fifth convolution\n",
    "\t\tx = self.conv5(x)\n",
    "\t\t# print(f'Shape after conv5: {x.shape}')\n",
    "\t\tx = self.relu5(x)\n",
    "\t\t# print(f'Shape after relu5: {x.shape}')\n",
    "\t\tx = self.maxpool3(x)\n",
    "\t\t# print(f'Shape after MP3: {x.shape}\\n')\n",
    "\n",
    "\t\t# Before passing to the fully connected layer we must flatten the tensor as nn.Linear expects a matrix (2d input)\n",
    "\t\tx = torch.flatten(x, start_dim=1)\n",
    "\t\t# print(x.shape)\n",
    "\n",
    "\t\t# First fully connected layer\n",
    "\t\tx = self.fc1(x)\n",
    "\t\t# print(f'Shape after fc1: {x.shape}')\n",
    "\t\tx = self.relu6(x)\n",
    "\t\t# print(f'Shape after relu6: {x.shape}')\n",
    "\t\tx = self.dropout1(x)\n",
    "\t\t# print(f'Shape after dropout1: {x.shape}\\n')\n",
    "\n",
    "\t\t# Second fully connected layer\n",
    "\t\tx = self.fc2(x)\n",
    "\t\t# print(f'Shape after fc2: {x.shape}')\n",
    "\t\tx = self.relu7(x)\n",
    "\t\t# print(f'Shape after relu7: {x.shape}')\n",
    "\t\tx = self.dropout2(x)\n",
    "\t\t# print(f'Shape after dropout2: {x.shape}\\n')\n",
    "\n",
    "\t\t# Final FC layer\n",
    "\t\tx = self.fc3(x)\n",
    "\t\t# print(f'Shape after fc3: {x.shape}')\n",
    "\t\t# x = self.relu8(x)\n",
    "\t\t# print(f'Shape after relu8: {x.shape}')\n",
    "\t\tx = self.softmax(x)\n",
    "\t\t# print(f'Shape after softmax: {x.shape}\\n')\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "model = AlexNet()\n",
    "\n",
    "if device is not None:\n",
    "\tmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964ea73014cde8bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:40.521083Z",
     "start_time": "2024-02-19T17:02:40.442557Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((227, 227)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "TRAIN_DATA_DIR = './Data/imagenette2-160/train'\n",
    "TEST_DATA_DIR = './Data/imagenette2-160/val'\n",
    "\n",
    "# Load ImageNette dataset\n",
    "trainset = torchvision.datasets.ImageFolder(\n",
    "        TRAIN_DATA_DIR, transform=transform\n",
    "    )\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "testset= torchvision.datasets.ImageFolder(\n",
    "        TEST_DATA_DIR, transform=transform\n",
    "    )\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e8a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23f9e9d64e1e7b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:07:37.883266Z",
     "start_time": "2024-02-19T17:02:41.821275Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35] starts...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::avg_pool3d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/t/Documents/GIT/ML Soc/QMML/my/Lesson 13/lesson_13_AlexNet.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/t/Documents/GIT/ML Soc/QMML/my/Lesson 13/lesson_13_AlexNet.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# print(f'Shape after ReLU1: {x.shape}')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLRN1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m# print(f'Shape after LRN1: {x.shape}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t/Documents/GIT/ML%20Soc/QMML/my/Lesson%2013/lesson_13_AlexNet.ipynb#X14sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool1(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/normalization.py:58\u001b[0m, in \u001b[0;36mLocalResponseNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlocal_response_norm(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta,\n\u001b[1;32m     59\u001b[0m                                  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2592\u001b[0m, in \u001b[0;36mlocal_response_norm\u001b[0;34m(input, size, alpha, beta, k)\u001b[0m\n\u001b[1;32m   2590\u001b[0m     div \u001b[39m=\u001b[39m div\u001b[39m.\u001b[39mview(sizes[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, sizes[\u001b[39m1\u001b[39m], sizes[\u001b[39m2\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   2591\u001b[0m     div \u001b[39m=\u001b[39m pad(div, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, (size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m))\n\u001b[0;32m-> 2592\u001b[0m     div \u001b[39m=\u001b[39m avg_pool3d(div, (size, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m), stride\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m   2593\u001b[0m     div \u001b[39m=\u001b[39m div\u001b[39m.\u001b[39mview(sizes)\n\u001b[1;32m   2594\u001b[0m div \u001b[39m=\u001b[39m div\u001b[39m.\u001b[39mmul(alpha)\u001b[39m.\u001b[39madd(k)\u001b[39m.\u001b[39mpow(beta)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::avg_pool3d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, weight_decay=0.0005, momentum=0.9)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "import time\n",
    "\n",
    "# for epoch in range(35):\n",
    "for epoch in range(1):\n",
    "    print(f'Epoch [{epoch + 1}/{35}] starts...')\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}] training loss: {train_loss:.3f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # Assuming test_loader is used as a validation loader\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(test_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f'Epoch [{epoch + 1}] validation loss: {val_loss:.3f}, accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Update the LR scheduler with validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    print(f'LR: {scheduler.get_last_lr()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f'Epoch [{epoch + 1}/{35}] ends. Time taken: {end_time - start_time:.2f} seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f8329f99c9ed1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
